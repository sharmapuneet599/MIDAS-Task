{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Task \n",
    "\n",
    "## Problem :\n",
    "In this problem we will be given images of 4 classes. The images and labels are separately converted into [pickle](https://docs.python.org/3/library/pickle.html) file. **Pickle** is used for serializing and de-serializing Python object structures, also called marshalling or flattening. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object. The dataset for this problem is available [here](https://drive.google.com/drive/folders/1F2PjpJ_u_iaD-Fs0wwcymRiVVLK34-Fu). Dataset contains 3 `pickle` files (training images, labels and test images respectively) and 1 `excel` file. We will put the predictions into the excel file in format given in the in the dataset's excel file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import pickle\n",
    "from torch import optim\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Now we will load the dataset. We will use the function [`pickle.load`](https://docs.python.org/3/library/pickle.html#pickle.load) to unpickle the data. We will unpickle training images and label and check of what datatype they are. We are also doing manual one hot endcoding into integers to make it easy for loss function to decrease the loss by looking at the encoded labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of training images: <class 'list'>\n",
      "No of Images: 8000\n",
      "Dimension of an Image: 784\n",
      "Datatype of training labels: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Loading training images\n",
    "Train = open('train_image.pkl','rb')\n",
    "images = pickle.load(Train)\n",
    "print ('Datatype of training images:',type(images))\n",
    "print('No of Images:',len(images))\n",
    "print('Dimension of an Image:',len(images[1]))\n",
    "\n",
    "# Loading labels\n",
    "label = open('train_label.pkl','rb')\n",
    "labels = pickle.load(label)\n",
    "print ('Datatype of training labels:',type(labels))\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 6:\n",
    "        labels[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Dataset Function\n",
    "We will make a Dataset Function which will do all the preprocessing to the data and this will help Data_loader by giving the images and labels of the particular index as desired by the Data_loader. Following preprocessing steps are necessary\n",
    "- We need to convert list into array as the data we have is in a list. So to convert it into a tensor we need to convert it into an array first, then convert it into a tensor. \n",
    "- We need to reshape the images as they are in shape 784. We will convert them into 28x28 with 1 channel.\n",
    "- We need to convert the pixel values into float as we need to normalize the pixels before feeding to the network because we dont want our network to deal matrix multiplications of large numbers.\n",
    "\n",
    "For more details on how to define your custom dataset function, go to [this guide](https://github.com/utkuozbulak/pytorch-custom-dataset-examples). It is a great guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, labels, transform = None):\n",
    "        \n",
    "        # Reshaping the image in format (no_of_images, height, width, channels) and converting their datatype to float \n",
    "        self.transform = transform\n",
    "        self.labels = np.asarray(labels)\n",
    "        self.images = np.asarray(images).reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        # Applying transformations\n",
    "        images, labels = self.images[item], self.labels[item]\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into Training and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data will have 1600 images\n"
     ]
    }
   ],
   "source": [
    "# Specifying the ratio in which we want to split data\n",
    "validation_split = 0.2\n",
    "\n",
    "# Setting shuffle = True to prevent training of data on same class \n",
    "shuffle_dataset = True\n",
    "\n",
    "# Getting the indices of training and validation dataset\n",
    "dataset_size = len(images)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "print('Validation data will have {} images'.format(split))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "# Defining training and validation indices \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Feeding training and validation indices to SubsetRandomSampler(This will randomly generate samples)\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Transformations and Defining training and validation Loader\n",
    "Now we will convert the data to a tensor and normalize it in range (-1,1). Then we will feed the dataset into my custom dataset function for all the preprocessing. Now we will use the function [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). It combines a dataset and a sampler, and provides single or multi-process iterators over the dataset. Then we are looking at a sample of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting data to a normalized torch.FloatTensor\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((127.5,), (127.5,))])\n",
    "\n",
    "# Feeding the dataset for all the preprocessing\n",
    "Data = MyDataset(images, labels, transform=train_transform)\n",
    "\n",
    "# Defining Train and validaion loader \n",
    "train_loader = torch.utils.data.DataLoader(Data, batch_size=50, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(Data, batch_size=50, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x238280051d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Having a visualisation of dataset\n",
    "photo, label = next(iter(train_loader))\n",
    "plt.imshow(photo[49].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining my own Network Architecture\n",
    "Here comes the main part of this project. All the cool and magical stuff happens here. We will use [Convolutional neural networks](https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8) here. CNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output. The whole network has a loss function and all the tips and tricks that we developed for neural networks still apply on CNNs. Most common layers we will be using are :\n",
    "\n",
    "- [Convolutional layers](https://pytorch.org/docs/stable/nn.html#conv2d), which can be thought of as stack of filtered images.\n",
    "- [Maxpooling layers](https://pytorch.org/docs/stable/nn.html#maxpool2d), which reduce the x-y size of an input, keeping only the most active pixels from the previous layer.\n",
    "- The usual Linear + Dropout layers to avoid overfitting and produce a 4-dim output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (Convolution1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (Convolution2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=1568, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.Convolution1 = nn.Sequential(\n",
    "            \n",
    "            # convolutional layer\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Batch Normalization layer\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            # Activation Function\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # max pooling layer\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "            )\n",
    "        \n",
    "        self.Convolution2 = nn.Sequential(\n",
    "            \n",
    "            # convolutional layer\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            \n",
    "            # Batch Normalization layer\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            # Activation Function\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # max pooling layer\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Dropout for regularization\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        #Fully Connected 1\n",
    "        self.fc = nn.Linear(32*7*7, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = self.Convolution1(x)\n",
    "        \n",
    "        output = self.Convolution2(output)\n",
    "        \n",
    "        output = output.view(output.size(0), -1)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# We will train model on GPU. So we need to move the model first to the GPU.\n",
    "model = CNN()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining [Loss Function](https://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and Training the model\n",
    "Here we will define Loss function which will calculate the loss and decide that how far we are from predicting the exact labels. Then we do backward pass to calculate the gradients. Then optimizer use these gradients to update the weights. Iterating these steps again and again, we train our model and make able to predict on unseen images or dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.535419 \tValidation Loss: 0.103237\n",
      "Validation loss decreased (inf --> 0.103237).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.402783 \tValidation Loss: 0.094551\n",
      "Validation loss decreased (0.103237 --> 0.094551).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.368264 \tValidation Loss: 0.095366\n",
      "Epoch: 4 \tTraining Loss: 0.335808 \tValidation Loss: 0.079976\n",
      "Validation loss decreased (0.094551 --> 0.079976).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.313619 \tValidation Loss: 0.079645\n",
      "Validation loss decreased (0.079976 --> 0.079645).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.307104 \tValidation Loss: 0.077837\n",
      "Validation loss decreased (0.079645 --> 0.077837).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.293809 \tValidation Loss: 0.077183\n",
      "Validation loss decreased (0.077837 --> 0.077183).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.276550 \tValidation Loss: 0.075385\n",
      "Validation loss decreased (0.077183 --> 0.075385).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.276352 \tValidation Loss: 0.074997\n",
      "Validation loss decreased (0.075385 --> 0.074997).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.265024 \tValidation Loss: 0.073909\n",
      "Validation loss decreased (0.074997 --> 0.073909).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.256200 \tValidation Loss: 0.073077\n",
      "Validation loss decreased (0.073909 --> 0.073077).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.239559 \tValidation Loss: 0.072917\n",
      "Validation loss decreased (0.073077 --> 0.072917).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.241348 \tValidation Loss: 0.072982\n",
      "Epoch: 14 \tTraining Loss: 0.231780 \tValidation Loss: 0.068707\n",
      "Validation loss decreased (0.072917 --> 0.068707).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.229008 \tValidation Loss: 0.072039\n",
      "Epoch: 16 \tTraining Loss: 0.219580 \tValidation Loss: 0.072289\n",
      "Epoch: 17 \tTraining Loss: 0.218127 \tValidation Loss: 0.069853\n",
      "Epoch: 18 \tTraining Loss: 0.203921 \tValidation Loss: 0.070016\n",
      "Epoch: 19 \tTraining Loss: 0.198482 \tValidation Loss: 0.071079\n",
      "Epoch: 20 \tTraining Loss: 0.192486 \tValidation Loss: 0.069048\n",
      "Epoch: 21 \tTraining Loss: 0.197093 \tValidation Loss: 0.069916\n",
      "Epoch: 22 \tTraining Loss: 0.195035 \tValidation Loss: 0.070263\n",
      "Epoch: 23 \tTraining Loss: 0.179204 \tValidation Loss: 0.068215\n",
      "Validation loss decreased (0.068707 --> 0.068215).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.178472 \tValidation Loss: 0.072371\n",
      "Epoch: 25 \tTraining Loss: 0.171181 \tValidation Loss: 0.074701\n",
      "Epoch: 26 \tTraining Loss: 0.176233 \tValidation Loss: 0.071823\n",
      "Epoch: 27 \tTraining Loss: 0.163469 \tValidation Loss: 0.072723\n",
      "Epoch: 28 \tTraining Loss: 0.163620 \tValidation Loss: 0.071718\n",
      "Epoch: 29 \tTraining Loss: 0.158403 \tValidation Loss: 0.075265\n",
      "Epoch: 30 \tTraining Loss: 0.148902 \tValidation Loss: 0.072213\n",
      "Epoch: 31 \tTraining Loss: 0.154676 \tValidation Loss: 0.073837\n",
      "Epoch: 32 \tTraining Loss: 0.149560 \tValidation Loss: 0.075577\n",
      "Epoch: 33 \tTraining Loss: 0.154326 \tValidation Loss: 0.070196\n",
      "Epoch: 34 \tTraining Loss: 0.145192 \tValidation Loss: 0.071492\n",
      "Epoch: 35 \tTraining Loss: 0.138704 \tValidation Loss: 0.074024\n",
      "Epoch: 36 \tTraining Loss: 0.134188 \tValidation Loss: 0.075939\n",
      "Epoch: 37 \tTraining Loss: 0.126418 \tValidation Loss: 0.073792\n",
      "Epoch: 38 \tTraining Loss: 0.133606 \tValidation Loss: 0.070708\n",
      "Epoch: 39 \tTraining Loss: 0.131078 \tValidation Loss: 0.074777\n",
      "Epoch: 40 \tTraining Loss: 0.133874 \tValidation Loss: 0.076615\n"
     ]
    }
   ],
   "source": [
    "# Using this torch.device we will move tensors to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Defining Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 40\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # Initiating of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Train the model \n",
    "\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        # Moving tensors to GPU if CUDA is available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device=device, dtype=torch.int64)\n",
    "        \n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Pass the images to the model and compute the predictions\n",
    "        output = model(images)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating the parameters by performing the optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update average training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for images, labels in validation_loader:\n",
    "        # Moving tensors to GPU if CUDA is available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device=device, dtype=torch.int64)\n",
    "     \n",
    "        # Forward pass: Pass the images to the model and compute the predictions\n",
    "        output = model(images)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Update validation loss \n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    # Calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(validation_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "     # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'MY_MODEL.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model weights\n",
    "model.load_state_dict(torch.load('MY_MODEL.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.068215\n",
      "\n",
      "Test Accuracy of     0: 85% (364/425)\n",
      "Test Accuracy of     2: 81% (324/399)\n",
      "Test Accuracy of     3: 88% (335/379)\n",
      "Test Accuracy of     6: 91% (365/397)\n",
      "\n",
      "Test Accuracy (Overall): 86% (1388/1600)\n"
     ]
    }
   ],
   "source": [
    "classes = ['0', '2', '3', '6']\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "\n",
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in validation_loader:\n",
    "    \n",
    "    # Moving tensors to GPU if CUDA is available\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device=device, dtype=torch.int64)\n",
    "    \n",
    "    # Forward pass: Pass the images to the model and compute the predictions\n",
    "    output = model(images)\n",
    "    \n",
    "    # Calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Update test loss \n",
    "    test_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    # Convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    \n",
    "    # Compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    # Calculate test accuracy for each object class\n",
    "    for i in range(50):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# Average test loss\n",
    "test_loss = test_loss/len(validation_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(4):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "Transfer learning is a popular method in computer vision because it allows us to build accurate models in a timesaving way. With transfer learning, instead of starting the learning process from scratch, you start from patterns that have been learned when solving a different problem. This way you leverage previous learnings and avoid starting from scratch.\n",
    "\n",
    "In computer vision, transfer learning is usually expressed through the use of pre-trained models. A pre-trained model is a model that was trained on a large benchmark dataset to solve a problem similar to the one that we want to solve. Accordingly, due to the computational cost of training such models, it is common practice to import and use models from published literature (e.g. VGG, Inception, MobileNet). Most of the pre-trained models are trained on the dataset of ImageNet.\n",
    "\n",
    "You can read more about Transfer Learning [here](https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751).\n",
    "\n",
    "**Note** : I will be using [Resnet18](https://arxiv.org/abs/1512.03385) model. I have picked the whole code of Resnet18 model here because I was unable to implement it for 1 channel by importing and there was no implementation for 1 channel available. So I have made few modifications in the layers to get it working for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=4, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet18()\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.519385 \tValidation Loss: 0.100400\n",
      "Validation loss decreased (inf --> 0.100400).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.358591 \tValidation Loss: 0.093487\n",
      "Validation loss decreased (0.100400 --> 0.093487).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.303957 \tValidation Loss: 0.081480\n",
      "Validation loss decreased (0.093487 --> 0.081480).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.276392 \tValidation Loss: 0.097263\n",
      "Epoch: 5 \tTraining Loss: 0.246952 \tValidation Loss: 0.108985\n",
      "Epoch: 6 \tTraining Loss: 0.233478 \tValidation Loss: 0.080530\n",
      "Validation loss decreased (0.081480 --> 0.080530).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.207622 \tValidation Loss: 0.097386\n",
      "Epoch: 8 \tTraining Loss: 0.195021 \tValidation Loss: 0.086608\n",
      "Epoch: 9 \tTraining Loss: 0.178416 \tValidation Loss: 0.104023\n",
      "Epoch: 10 \tTraining Loss: 0.159108 \tValidation Loss: 0.118279\n",
      "Epoch: 11 \tTraining Loss: 0.158361 \tValidation Loss: 0.093051\n",
      "Epoch: 12 \tTraining Loss: 0.128245 \tValidation Loss: 0.112150\n",
      "Epoch: 13 \tTraining Loss: 0.125312 \tValidation Loss: 0.101854\n",
      "Epoch: 14 \tTraining Loss: 0.117011 \tValidation Loss: 0.100135\n",
      "Epoch: 15 \tTraining Loss: 0.096623 \tValidation Loss: 0.109354\n",
      "Epoch: 16 \tTraining Loss: 0.101031 \tValidation Loss: 0.122750\n",
      "Epoch: 17 \tTraining Loss: 0.088087 \tValidation Loss: 0.124515\n",
      "Epoch: 18 \tTraining Loss: 0.075151 \tValidation Loss: 0.114578\n",
      "Epoch: 19 \tTraining Loss: 0.067285 \tValidation Loss: 0.128500\n",
      "Epoch: 20 \tTraining Loss: 0.065620 \tValidation Loss: 0.127309\n",
      "Epoch: 21 \tTraining Loss: 0.070787 \tValidation Loss: 0.113800\n",
      "Epoch: 22 \tTraining Loss: 0.055806 \tValidation Loss: 0.142951\n",
      "Epoch: 23 \tTraining Loss: 0.065324 \tValidation Loss: 0.138360\n",
      "Epoch: 24 \tTraining Loss: 0.052733 \tValidation Loss: 0.121494\n",
      "Epoch: 25 \tTraining Loss: 0.050981 \tValidation Loss: 0.131812\n",
      "Epoch: 26 \tTraining Loss: 0.034110 \tValidation Loss: 0.137241\n",
      "Epoch: 27 \tTraining Loss: 0.049497 \tValidation Loss: 0.134210\n",
      "Epoch: 28 \tTraining Loss: 0.048823 \tValidation Loss: 0.136233\n",
      "Epoch: 29 \tTraining Loss: 0.033896 \tValidation Loss: 0.138770\n",
      "Epoch: 30 \tTraining Loss: 0.037282 \tValidation Loss: 0.159699\n",
      "Epoch: 31 \tTraining Loss: 0.047579 \tValidation Loss: 0.126545\n",
      "Epoch: 32 \tTraining Loss: 0.036463 \tValidation Loss: 0.149165\n",
      "Epoch: 33 \tTraining Loss: 0.029671 \tValidation Loss: 0.170154\n",
      "Epoch: 34 \tTraining Loss: 0.034094 \tValidation Loss: 0.160857\n",
      "Epoch: 35 \tTraining Loss: 0.028901 \tValidation Loss: 0.170801\n",
      "Epoch: 36 \tTraining Loss: 0.040899 \tValidation Loss: 0.146521\n",
      "Epoch: 37 \tTraining Loss: 0.033556 \tValidation Loss: 0.149450\n",
      "Epoch: 38 \tTraining Loss: 0.023035 \tValidation Loss: 0.166164\n",
      "Epoch: 39 \tTraining Loss: 0.044477 \tValidation Loss: 0.130386\n",
      "Epoch: 40 \tTraining Loss: 0.036780 \tValidation Loss: 0.128434\n"
     ]
    }
   ],
   "source": [
    "# Using this torch.device we will move tensors to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Defining Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 40\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # Initiating of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Train the model \n",
    "\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        # Moving tensors to GPU if CUDA is available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device=device, dtype=torch.int64)\n",
    "        \n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: Pass the images to the model and compute the predictions\n",
    "        output = model(images)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating the parameters by performing the optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update average training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        \n",
    "    model.eval()\n",
    "    for images, labels in validation_loader:\n",
    "        # Moving tensors to GPU if CUDA is available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device=device, dtype=torch.int64)\n",
    "     \n",
    "        # Forward pass: Pass the images to the model and compute the predictions\n",
    "        output = model(images)\n",
    "        \n",
    "        # Calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # Update validation loss \n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    # Calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(validation_loader.dataset)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "     # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'Transferlearning.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('Transferlearning.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.080530\n",
      "\n",
      "Test Accuracy of     0: 81% (346/425)\n",
      "Test Accuracy of     2: 69% (276/399)\n",
      "Test Accuracy of     3: 92% (349/379)\n",
      "Test Accuracy of     6: 90% (360/397)\n",
      "\n",
      "Test Accuracy (Overall): 83% (1331/1600)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "classes = ['0', '2', '3', '6']\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(4))\n",
    "class_total = list(0. for i in range(4))\n",
    "\n",
    "# Check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Iterate over test data\n",
    "for images, labels in validation_loader:\n",
    "    \n",
    "    # Moving tensors to GPU if CUDA is available\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device=device, dtype=torch.int64)\n",
    "    \n",
    "    # Forward pass: Pass the images to the model and compute the predictions\n",
    "    output = model(images)\n",
    "    \n",
    "    # Calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    \n",
    "    # Update test loss \n",
    "    test_loss += loss.item()*images.size(0)\n",
    "    \n",
    "    # Convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)\n",
    "    \n",
    "    # Compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    # Calculate test accuracy for each object class\n",
    "    for i in range(50):\n",
    "        label = labels.data[i]\n",
    "        class_correct[label] += correct[i].item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "# Average test loss\n",
    "test_loss = test_loss/len(validation_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(4):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test = open('test_image.pkl','rb')\n",
    "testimages = pickle.load(Test)\n",
    "len(testimages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying MyDataset Function\n",
    "We need to remove the label preprocessing from the function as we do not have test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, images, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Reshaping the image in format (no_of_images, height, width, channels) and converting their datatype to float\n",
    "        self.images = np.asarray(images).reshape(-1, 28, 28, 1).astype('float32')\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        images = self.images[item]\n",
    "        \n",
    "        # Applying transformation to test images\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(images)\n",
    "\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "# Converting data to a normalized torch.FloatTensor\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((127.5,), (127.5,))])\n",
    "\n",
    "# Feeding the test images for all the preprocessing\n",
    "Data1 = MyDataset(testimages, transform=test_transform)\n",
    "\n",
    "# Defining Test loader \n",
    "test_loader = torch.utils.data.DataLoader(Data1, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Feeding the test images through the model and saving the predictions in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model):\n",
    "    # List that will store the predictions\n",
    "    result = []\n",
    "    # Disabling the calculation of gradients because we are testing\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Fetching images from testloader\n",
    "        for  test in test_loader:\n",
    "            \n",
    "            # Importing the images to GPU\n",
    "            test = test.to(device)\n",
    "            \n",
    "            # Feeding test images into model\n",
    "            outputs = model(test)\n",
    "            \n",
    "            # Making the probabilities into numbers\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            \n",
    "            # Storing the predictions in the list\n",
    "            for i in range (len(test)):\n",
    "                temp = int(predicted[i].cpu().numpy())\n",
    "                result.append(temp)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = testing(model)\n",
    "# Decoding the encoded labels\n",
    "for i in range(len(result)):\n",
    "    if result[i] == 1:\n",
    "        result[i] = 6\n",
    "        \n",
    "# List for the index of images\n",
    "a = []\n",
    "for i in range(2000):\n",
    "    a.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting the predicitons to the .CSV file\n",
    "pd.DataFrame({'image_index':a,'class':result}).to_csv(r'C:\\Users\\sharm\\OneDrive\\Desktop\\CNN\\Puneet_Sharma.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
